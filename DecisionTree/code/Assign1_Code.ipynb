{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66417358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import numpy\n",
    "from random import randrange\n",
    "import math\n",
    "from collections import Counter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4878bbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "myname = \"Tuhin-Dutta_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8347bc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_decision_tree():\n",
    "    # Load data set\n",
    "    file_name = os.path.join(os.getcwd(), \"wine-dataset.csv\")\n",
    "    df = pd.read_csv(file_name)\n",
    "    # remove the quality column\n",
    "    trainSet = df[df.columns[:-1]]\n",
    "    # normalize all data columns\n",
    "    trainSet = (trainSet - trainSet.min())/(trainSet.max() - trainSet.min())\n",
    "    # get the quality column labels\n",
    "    trainLabel = df.iloc[:,-1:]\n",
    "    # concat trainSet with trainLabel to create final DataFrame df\n",
    "    df = pd.concat([trainSet, trainLabel], axis=1)\n",
    "    #declare K for k fold\n",
    "    K = 9 #folds\n",
    "    # calculate fold size\n",
    "    fold_size = int(trainSet.shape[0]/K)\n",
    "    # divide train set into K folds each of size fold_size\n",
    "    folds = [df.iloc[i:i+fold_size] for i in range(0,len(df)-fold_size+1, fold_size)] \n",
    "    # Get all attributes names\n",
    "    attributes = list(df.columns)\n",
    "    # Remove the class attribute\n",
    "    attributes.remove('quality')\n",
    "    tot_acc = 0\n",
    "    for k in range(K):\n",
    "        test = folds[k] # validation set\n",
    "        training = [rows for j,rows in enumerate(folds) if k != j] # k-fold training set\n",
    "        train_df = pd.DataFrame()\n",
    "        # append all the k-folds into one data frame\n",
    "        for i in range(int(8)):\n",
    "            train_df = pd.concat([train_df, training[i]])\n",
    "        # call learn method of tree\n",
    "        train_tree = learn(train_df, 'quality', attributes)\n",
    "        # predict\n",
    "        test['predicted'] = test.apply(classify,axis=1,args=(train_tree,1))\n",
    "        # evaluate accuracy\n",
    "        acc = sum(test['quality'] == test['predicted'] ) / (1.0*len(test.index))\n",
    "        # total accuracy over k-folds\n",
    "        tot_acc += acc\n",
    "    \n",
    "    # Writing results to a file (DO NOT CHANGE)\n",
    "    f = open(myname+\"result.txt\", \"a\")\n",
    "    #f.write(\"Using Entropy, Accuracy: %.4f\\n\" % float(tot_acc/(1.0*K)))\n",
    "    f.write(\"Using Gini, Accuracy: %.4f\\n\" % float(tot_acc/(1.0*K)))\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf38444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(df,target,attributes,default_class = 0):\n",
    "    # count distinct target values\n",
    "    count = Counter(x for x in df[target])\n",
    "    if len(count) == 1:\n",
    "        return next(iter(count))\n",
    "    elif df.empty or (not attributes):\n",
    "        return default_class\n",
    "    else:\n",
    "        default_class = max(count.keys())\n",
    "        # calculate information gain using Entropy\n",
    "        \"\"\"Comment/Uncomment out the below gain if using entropy\"\"\"\n",
    "        #gains = [info_gain_entropy(df,attr,target) for attr in attributes]\n",
    "        \n",
    "        \"\"\"Comment/Uncomment out the below gain if using gini\"\"\"\n",
    "        # calculate information gain using Gini Impurity\n",
    "        gains = [info_gain_gini(df,attr,target) for attr in attributes]\n",
    "        \n",
    "        # arg max of gains\n",
    "        index_max = gains.index(max(gains))\n",
    "        # get best attribute at the current node\n",
    "        best_attr = attributes[index_max]\n",
    "        # create new tree\n",
    "        tree = { best_attr:{ } }\n",
    "        # get remaining attributes\n",
    "        remaining_attr = [x for x in attributes if x != best_attr]\n",
    "        \n",
    "        for attr_val, data_subset in df.groupby(best_attr):\n",
    "            subtree = learn(data_subset,target,remaining_attr,default_class) # recursive call to build tree\n",
    "            tree[best_attr][attr_val] = subtree\n",
    "        return tree\n",
    "    \n",
    "def classify(instance,tree,default = 0):\n",
    "    attribute = next(iter(tree))\n",
    "    if instance[attribute] in tree[attribute].keys():\n",
    "        result = tree[attribute][instance[attribute]]\n",
    "        if isinstance(result,dict):\n",
    "            return classify(instance,result) # recursive call to classify instance \n",
    "        else:\n",
    "            return result\n",
    "    else:\n",
    "        return default\n",
    "\n",
    "def calcEntropy(prob):\n",
    "    return sum([-p*math.log(p,2) for p in prob])\n",
    "\n",
    "def info_gain_entropy(df,split,target,trace=0):\n",
    "    df_split = df.groupby(split)\n",
    "    n = len(df.index)*1.0\n",
    "    df_agg_ent = df_split.agg({ target:[getEntropyList, lambda x: len(x)/n] })\n",
    "    df_agg_ent.columns = ['E','PO']\n",
    "    new_E = sum( df_agg_ent['E'] * df_agg_ent[\"PO\"])\n",
    "    old_E = getEntropyList(df[target])\n",
    "    return old_E - new_E\n",
    "\n",
    "\n",
    "def getEntropyList(l):\n",
    "    count = Counter(x for x in l)\n",
    "    n = len(l)*1.0\n",
    "    p = [x/n for x in count.values()]\n",
    "    return calcEntropy(p)\n",
    "\n",
    "def calcGini(rows):\n",
    "    counts = Counter(x for x in rows)\n",
    "    im = 1\n",
    "    for lbl in counts:\n",
    "        prob_of_lbl = counts[lbl] / float(len(rows))\n",
    "        im -= prob_of_lbl**2\n",
    "    return im\n",
    "\n",
    "def info_gain_gini(l, r, curr):\n",
    "    p = float(len(l)) / (len(l) + len(r))\n",
    "    return calcGini(curr) - p * calcGini(l) - (1 - p) * calcGini(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9616374",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yq/qh4ny6ms3kz0yqqj4q2gqkn00000gn/T/ipykernel_8223/2298038914.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['predicted'] = test.apply(classify,axis=1,args=(train_tree,1))\n"
     ]
    }
   ],
   "source": [
    "run_decision_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5d8764",
   "metadata": {},
   "source": [
    "We saw our accuracy improved from 0.6863 using Entropy to 0.7433 using Gini.\n",
    "The Gini Index and the Entropy have two main differences:\n",
    "1. Gini Index has values inside the interval [0, 0.5] (Gini’s maximum impurity is 0.5 and maximum purity is 0) whereas the interval of the Entropy is [0, 1] (Entropy’s maximum impurity is 1 and maximum purity is 0). \n",
    "2. Computationally, entropy is more complex since it makes use of logarithms and consequently, the calculation of the Gini Index will be faster. Entropy is more computationally heavy due to the log in the equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f8dcd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
