{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RandomForest.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM6ZwvcggtJO+xovbVjnImV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tauhait/ml-projects/blob/main/RandomForest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADzDx7Hf1ZN7"
      },
      "source": [
        "https://www.python-course.eu/Random_Forests.php"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYxtfg6A1bcA"
      },
      "source": [
        "\"\"\"\n",
        "Make the imports of python packages needed\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "import scipy.stats as sps\n",
        "\n",
        "\n",
        "\n",
        "dataset = pd.read_csv('mushroom.csv',header=None)\n",
        "dataset = dataset.sample(frac=1)\n",
        "dataset.columns = ['target','cap-shape','cap-surface','cap-color','bruises','odor','gill-attachment','gill-spacing',\n",
        "             'gill-size','gill-color','stalk-shape','stalk-root','stalk-surface-above-ring','stalk-surface-below-ring','stalk-color-above-ring',\n",
        "             'stalk-color-below-ring','veil-type','veil-color','ring-number','ring-type','spore-print-color','population',\n",
        "             'habitat']"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3r0ttE75uE7"
      },
      "source": [
        "def entropy(target_col):\n",
        "    elements,counts = np.unique(target_col,return_counts = True)\n",
        "    entropy = np.sum([(-counts[i]/np.sum(counts))*np.log2(counts[i]/np.sum(counts)) for i in range(len(elements))])\n",
        "    return entropy\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxjRAPq36M6f"
      },
      "source": [
        "def InfoGain(data,split_attribute_name,target_name=\"target\"):\n",
        "    \n",
        "    #Calculate the entropy of the total dataset\n",
        "    total_entropy = entropy(data[target_name])\n",
        "    \n",
        "    ##Calculate the entropy of the dataset\n",
        "    \n",
        "    #Calculate the values and the corresponding counts for the split attribute \n",
        "    vals,counts= np.unique(data[split_attribute_name],return_counts=True)\n",
        "    \n",
        "    #Calculate the weighted entropy\n",
        "    Weighted_Entropy = np.sum([(counts[i]/np.sum(counts))*entropy(data.where(data[split_attribute_name]==vals[i]).dropna()[target_name]) for i in range(len(vals))])\n",
        "    \n",
        "    #Calculate the information gain\n",
        "    Information_Gain = total_entropy - Weighted_Entropy\n",
        "    return Information_Gain"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vt31K9bV6WUJ"
      },
      "source": [
        "def ID3(data,originaldata,features,target_attribute_name=\"target\",parent_node_class = None):\n",
        "    #Define the stopping criteria --> If one of this is satisfied, we want to return a leaf node#\n",
        "    \n",
        "    #If all target_values have the same value, return this value\n",
        "    if len(np.unique(data[target_attribute_name])) <= 1:\n",
        "        return np.unique(data[target_attribute_name])[0]\n",
        "    \n",
        "    #If the dataset is empty, return the mode target feature value in the original dataset\n",
        "    elif len(data)==0:\n",
        "        return np.unique(originaldata[target_attribute_name])[np.argmax(np.unique(originaldata[target_attribute_name],return_counts=True)[1])]\n",
        "    \n",
        "    #If the feature space is empty, return the mode target feature value of the direct parent node --> Note that\n",
        "    #the direct parent node is that node which has called the current run of the ID3 algorithm and hence\n",
        "    #the mode target feature value is stored in the parent_node_class variable.\n",
        "    \n",
        "    elif len(features) ==0:\n",
        "        return parent_node_class\n",
        "    \n",
        "    #If none of the above holds true, grow the tree!\n",
        "    \n",
        "    else:\n",
        "        #Set the default value for this node --> The mode target feature value of the current node\n",
        "        parent_node_class = np.unique(data[target_attribute_name])[np.argmax(np.unique(data[target_attribute_name],return_counts=True)[1])]\n",
        "        \n",
        "        \n",
        "        ################################################################################################################\n",
        "        ############!!!!!!!!!Implement the subspace sampling. Draw a number of m = sqrt(p) features!!!!!!!!#############\n",
        "        ###############################################################################################################\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        features = np.random.choice(features,size=np.int(np.sqrt(len(features))),replace=False)\n",
        "        \n",
        "        #Select the feature which best splits the dataset\n",
        "        item_values = [InfoGain(data,feature,target_attribute_name) for feature in features] #Return the information gain values for the features in the dataset\n",
        "        best_feature_index = np.argmax(item_values)\n",
        "        best_feature = features[best_feature_index]\n",
        "        \n",
        "        #Create the tree structure. The root gets the name of the feature (best_feature) with the maximum information\n",
        "        #gain in the first run\n",
        "        tree = {best_feature:{}}\n",
        "        \n",
        "        #Remove the feature with the best inforamtion gain from the feature space\n",
        "        features = [i for i in features if i != best_feature]\n",
        "        \n",
        "        \n",
        "        #Grow a branch under the root node for each possible value of the root node feature\n",
        "        \n",
        "        for value in np.unique(data[best_feature]):\n",
        "            value = value\n",
        "            #Split the dataset along the value of the feature with the largest information gain and therwith create sub_datasets\n",
        "            sub_data = data.where(data[best_feature] == value).dropna()\n",
        "            \n",
        "            #Call the ID3 algorithm for each of those sub_datasets with the new parameters --> Here the recursion comes in!\n",
        "            subtree = ID3(sub_data,dataset,features,target_attribute_name,parent_node_class)\n",
        "            \n",
        "            #Add the sub tree, grown from the sub_dataset to the tree under the root node\n",
        "            tree[best_feature][value] = subtree\n",
        "            \n",
        "        return(tree)    "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBay-Oer6y58"
      },
      "source": [
        "def predict(query,tree,default = 'p'):\n",
        "        \n",
        "    for key in list(query.keys()):\n",
        "        if key in list(tree.keys()):\n",
        "            try:\n",
        "                result = tree[key][query[key]] \n",
        "            except:\n",
        "                return default\n",
        "            result = tree[key][query[key]]\n",
        "            if isinstance(result,dict):\n",
        "                return predict(query,result)\n",
        "\n",
        "            else:\n",
        "                return result"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rpfspw_B63Dn"
      },
      "source": [
        "def train_test_split(dataset):\n",
        "    training_data = dataset.iloc[:round(0.75*len(dataset))].reset_index(drop=True)#We drop the index respectively relabel the index\n",
        "    #starting form 0, because we do not want to run into errors regarding the row labels / indexes\n",
        "    testing_data = dataset.iloc[round(0.75*len(dataset)):].reset_index(drop=True)\n",
        "    return training_data,testing_data\n",
        "\n",
        "\n",
        "training_data = train_test_split(dataset)[0]\n",
        "testing_data = train_test_split(dataset)[1] "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uo2j4rR65kO"
      },
      "source": [
        "#######Train the Random Forest model###########\n",
        "\n",
        "def RandomForest_Train(dataset,number_of_Trees):\n",
        "    #Create a list in which the single forests are stored\n",
        "    random_forest_sub_tree = []\n",
        "    \n",
        "    #Create a number of n models\n",
        "    for i in range(number_of_Trees):\n",
        "        #Create a number of bootstrap sampled datasets from the original dataset \n",
        "        bootstrap_sample = dataset.sample(frac=1,replace=True)\n",
        "        \n",
        "        #Create a training and a testing datset by calling the train_test_split function\n",
        "        bootstrap_training_data = train_test_split(bootstrap_sample)[0]\n",
        "        bootstrap_testing_data = train_test_split(bootstrap_sample)[1] \n",
        "        \n",
        "        \n",
        "        #Grow a tree model for each of the training data\n",
        "        #We implement the subspace sampling in the ID3 algorithm itself. Hence take a look at the ID3 algorithm above!\n",
        "        random_forest_sub_tree.append(ID3(bootstrap_training_data,bootstrap_training_data,bootstrap_training_data.drop(labels=['target'],axis=1).columns))\n",
        "        \n",
        "    return random_forest_sub_tree\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EK93unu76-hh"
      },
      "source": [
        "random_forest = RandomForest_Train(dataset,50)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VO7EZRPo7B0i"
      },
      "source": [
        "#######Test the model on the testing data and return the accuracy###########\n",
        "def RandomForest_Test(data,random_forest):\n",
        "    data['predictions'] = None\n",
        "    for i in range(len(data)):\n",
        "        query = data.iloc[i,:].drop('target').to_dict()\n",
        "        data.loc[i,'predictions'] = RandomForest_Predict(query,random_forest,default='p')\n",
        "    accuracy = sum(data['predictions'] == data['target'])/len(data)*100\n",
        "    #print('The prediction accuracy is: ',sum(data['predictions'] == data['target'])/len(data)*100,'%')\n",
        "    return accuracy"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0kL0JkN7S7W",
        "outputId": "8539a6ee-c340-442b-f3ab-3ed8b5f03f63"
      },
      "source": [
        "#######Predict a new query instance###########\n",
        "def RandomForest_Predict(query,random_forest,default='p'):\n",
        "    predictions = []\n",
        "    for tree in random_forest:\n",
        "        predictions.append(predict(query,tree,default))\n",
        "    return sps.mode(predictions)[0][0]\n",
        "\n",
        "\n",
        "query = testing_data.iloc[0,:].drop('target').to_dict()\n",
        "query_target = testing_data.iloc[0,0]\n",
        "print('target: ',query_target)\n",
        "prediction = RandomForest_Predict(query,random_forest)\n",
        "print('prediction: ',prediction)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "target:  e\n",
            "prediction:  e\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERVJuui67Ztl"
      },
      "source": [
        "#######Test the model on the testing data and return the accuracy###########\n",
        "def RandomForest_Test(data,random_forest):\n",
        "    data['predictions'] = None\n",
        "    for i in range(len(data)):\n",
        "        query = data.iloc[i,:].drop('target').to_dict()\n",
        "        data.loc[i,'predictions'] = RandomForest_Predict(query,random_forest,default='p')\n",
        "    accuracy = sum(data['predictions'] == data['target'])/len(data)*100\n",
        "    #print('The prediction accuracy is: ',sum(data['predictions'] == data['target'])/len(data)*100,'%')\n",
        "    return accuracy"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gx6qul-v78ON",
        "outputId": "aa57d670-196b-472a-d32e-31244ee16d1d"
      },
      "source": [
        "RandomForest_Test(testing_data,random_forest)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "90.89118660758247"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVfz6f3y7_Mg",
        "outputId": "8de5d79d-ea4f-4498-ee56-779aa3611dac"
      },
      "source": [
        "###Random Forests using sklearn\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "#Encode the feature values which are strings to integers\n",
        "for label in dataset.columns:\n",
        "    dataset[label] = LabelEncoder().fit(dataset[label]).transform(dataset[label])\n",
        "\n",
        "\n",
        "X = dataset.drop(['target'],axis=1)\n",
        "Y = dataset['target']\n",
        "\n",
        "\n",
        "#Instantiate the model with 100 trees and entropy as splitting criteria\n",
        "Random_Forest_model = RandomForestClassifier(n_estimators=100,criterion=\"entropy\")\n",
        "\n",
        "\n",
        "#Cross validation\n",
        "accuracy = cross_validate(Random_Forest_model,X,Y,cv=10)['test_score']\n",
        "print('The accuracy is: ',sum(accuracy)/len(accuracy)*100,'%')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy is:  100.0 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVxLOCsi8LtA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}